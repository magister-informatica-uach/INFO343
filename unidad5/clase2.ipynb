{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib notebook\n",
    "from  tqdm import tqdm_notebook\n",
    "#import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting MNIST using `torchvision.datasets`\n",
    "\n",
    "In this case images are in [PIL](https://pillow.readthedocs.io/en/stable/) format, so we have to transform them to torch `tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "mnist_train_data = torchvision.datasets.MNIST('dataset', train=True, download=True,\n",
    "                                              transform=torchvision.transforms.ToTensor())\n",
    "mnist_test_data = torchvision.datasets.MNIST('dataset', train=False, download=True,\n",
    "                                             transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "image, label = mnist_train_data[0]\n",
    "print(image.shape)  # Channels, width, height\n",
    "print(type(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative: Build dataset from numpy matrix with `TensorDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.datasets import fetch_openml\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "mnist_train_data = TensorDataset(torch.from_numpy(X_train.astype('float32')),\n",
    "                                 torch.from_numpy(y_train.astype('int64')))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 10, figsize=(10, 2))\n",
    "for k in range(10):\n",
    "    image, label = mnist_train_data[k]\n",
    "    #print(image.numpy().shape)\n",
    "    ax[k].imshow(np.array(image.numpy()[0]), cmap=plt.cm.Greys_r)\n",
    "    ax[k].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Torch Dataloaders. This will push the data to the neural net during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "mnist_train_loader = DataLoader(mnist_train_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an artificial neural network model using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Simple_convnet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=10):\n",
    "        super(Simple_convnet, self).__init__()\n",
    "        #self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=n_conv, kernel_size=3, stride=1, bias=True)\n",
    "        #self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        #self.conv2 = torch.nn.Conv2d(in_channels=n_conv, out_channels=n_conv, kernel_size=3, stride=1, bias=True)\n",
    "        #self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = torch.nn.Linear(in_features=28*28,\n",
    "                                   out_features=n_hidden, bias=True) #n_conv*5*5\n",
    "        self.fc2 = torch.nn.Linear(in_features=n_hidden,\n",
    "                                   out_features=10, bias=True)\n",
    "        self.hidden_activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #z = F.relu(self.conv1(x.view(-1, 1, 28, 28)))\n",
    "        #z = self.pool1(z)\n",
    "        #z = F.relu(self.conv2(z))\n",
    "        #z = self.pool2(z)\n",
    "        #z = z.view(-1, net.fc1.in_features)\n",
    "        z = x.view(-1, 28*28)\n",
    "        z = self.hidden_activation(self.fc1(z))\n",
    "        z = F.log_softmax(self.fc2(z), dim=1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform training, we will use [hiddenlayer](https://github.com/waleedka/hiddenlayer) to track training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hiddenlayer as hl\n",
    "my_conv_nnet = Simple_convnet()\n",
    "# hl.build_graph(my_conv_nnet, torch.zeros([1, 3, 224, 224])) requires graphviz\n",
    "# https://github.com/waleedka/hiddenlayer/blob/master/demos/pytorch_graph.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_conv_nnet = Simple_convnet()\n",
    "nepochs = 5\n",
    "optimizer = torch.optim.Adam(my_conv_nnet.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "#net.to(device)\n",
    "\n",
    "# Hiddenlayer objects to track metrics\n",
    "history1 = hl.History()\n",
    "canvas1 = hl.Canvas()\n",
    "\n",
    "for epoch in range(nepochs): \n",
    "    epoch_loss, epoch_acc = 0.0, 0.0\n",
    "    for data, target in mnist_train_loader:\n",
    "        #data, target = data.to(device), target.to(device)\n",
    "        prediction = my_conv_nnet(data)\n",
    "        optimizer.zero_grad()        \n",
    "        loss = criterion(prediction, target)  \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (prediction.argmax(dim=1) == target).sum().item()\n",
    "        if epoch > 0:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    # Do this every X epochs:\n",
    "    history1.log(epoch, loss=epoch_loss, accuracy=epoch_acc/mnist_train_data.__len__())\n",
    "    with canvas1: # So that they render together\n",
    "        canvas1.draw_plot([history1[\"loss\"]])\n",
    "        canvas1.draw_plot([history1[\"accuracy\"]])\n",
    "    #time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow version"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "\n",
    "npix = 21\n",
    "batch_size = 128\n",
    "n_batches = 100\n",
    "nepochs = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf_input = tf.placeholder(tf.float32, [None, npix, npix, 1], name='input')\n",
    "tf_label = tf.placeholder(tf.int64, None, name='target')\n",
    "\"\"\"\n",
    "with tf.variable_scope('conv_layer1'):\n",
    "    z = tf.layers.conv2d(inputs=tf_input, filters=64, kernel_size=5, strides=1, \n",
    "                         use_bias=True, activation=tf.nn.relu)\n",
    "    z = tf.layers.max_pooling2d(inputs=z, pool_size=2, strides=2)\n",
    "    \n",
    "with tf.variable_scope('conv_layer2'):\n",
    "    z = tf.layers.conv2d(inputs=z, filters=64, kernel_size=5, strides=2, \n",
    "                         use_bias=True, activation=tf.nn.relu)\n",
    "    z = tf.layers.max_pooling2d(inputs=z, pool_size=2, strides=2)\n",
    "\"\"\"\n",
    "with tf.variable_scope('fc_layer1'):\n",
    "    z = tf.layers.dense(tf.layers.flatten(tf_input), units=128, activation=tf.nn.tanh)\n",
    "\n",
    "with tf.variable_scope('output_layer'):\n",
    "    y = tf.layers.dense(z, units=1, activation=None)\n",
    "\n",
    "with tf.variable_scope('Optimizer'):\n",
    "    global_step_train = tf.Variable(0, name='global_step', trainable=False)  \n",
    "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(tf_label, tf.float32), \n",
    "                                                            logits=y)\n",
    "    loss_op = tf.reduce_mean(cross_entropy)  \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=global_step_train) \n",
    "    #init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "    correct_prediction = tf.equal(tf_label, tf.argmax(tf.nn.sigmoid(y), 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('loss', loss_op)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With TFrecord data format"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "log_dir = str(time.time())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer_train = tf.summary.FileWriter(join(log_dir, 'train'), sess.graph)\n",
    "    writer_val = tf.summary.FileWriter(join(log_dir, 'val'), sess.graph)\n",
    "    \n",
    "    with tf.variable_scope('train_set'):\n",
    "        dataset_train = tf.data.TFRecordDataset(path_val)\n",
    "        dataset_train = dataset_train.map(decode)\n",
    "        dataset_train = dataset_train.map(normalize_minmax)\n",
    "        dataset_train = dataset_train.batch(batch_size)\n",
    "        dataset_train = dataset_train.shuffle(buffer_size=3*batch_size)\n",
    "        #dataset_train = dataset_train.repeat()\n",
    "        iterator_train = dataset_train.make_initializable_iterator()\n",
    "        next_element_train = iterator_train.get_next()\n",
    "        \n",
    "    with tf.variable_scope('val_set'):\n",
    "        dataset_val = tf.data.TFRecordDataset(path_val)\n",
    "        dataset_val = dataset_val.map(decode)\n",
    "        dataset_val = dataset_val.map(normalize_minmax)\n",
    "        dataset_val = dataset_val.batch(10000)\n",
    "        #dataset_val = dataset_val.repeat()\n",
    "        iterator_val = dataset_val.make_initializable_iterator()\n",
    "        next_element_val = iterator_val.get_next()\n",
    "    \n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    for k in tqdm_notebook(range(nepochs), desc='Train epoch'):\n",
    "        sess.run(iterator_train.initializer)\n",
    "        for batch_index in range(n_batches): # Train\n",
    "            img, lbl = sess.run(next_element_train)\n",
    "            img = img[:, :, :, 3, np.newaxis]\n",
    "            _, summaries, step = sess.run([train_op, merged, global_step_train], \n",
    "                                          feed_dict={tf_input: img, tf_label: lbl})\n",
    "            writer_train.add_summary(summaries, global_step=step)\n",
    "        sess.run(iterator_val.initializer)\n",
    "        img, lbl = sess.run(next_element_val)\n",
    "        img = img[:, :, :, 3, np.newaxis]\n",
    "        _, summaries, acc = sess.run([loss_op, merged, accuracy], \n",
    "                                     feed_dict={tf_input: img, tf_label: lbl})\n",
    "        print(\"%d %f\" %(k, acc))\n",
    "        writer_val.add_summary(summaries, global_step=k)\n",
    "        y_val = sess.run(y, feed_dict={tf_input: img, tf_label: lbl})\n",
    "        \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
