{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from tqdm import tqdm_notebook\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset MNIST con `torchvision`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST es un dataset de dígitos manuscritos con 70.000 ejemplos compuesto por Yann Lecun y usado como benchmark para algoritmos de visión computacional desde 1998: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "Podemos usar el módulo [`torchvision.datasets`](https://pytorch.org/docs/stable/torchvision/datasets.html) para descargar MNIST y otros datasets típicos de benchmark. Los datasets heredan de `torch.utils.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_data = torchvision.datasets.MNIST('dataset', train=True, download=True, \n",
    "                                              transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "image, label = mnist_train_data[0]\n",
    "display(len(mnist_train_data), type(image), type(label))\n",
    "fig, ax = plt.subplots(1, 10, figsize=(8, 2), tight_layout=True)\n",
    "for k in range(10):\n",
    "    image, label = mnist_train_data[k]\n",
    "    ax[k].imshow(image.numpy()[0, :, :], cmap=plt.cm.Greys_r)\n",
    "    ax[k].axis('off');\n",
    "    ax[k].set_title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por defecto los labels están en tipo entero y las imágenes en formato [PIL/Pillow](https://pillow.readthedocs.io/en/stable/)\n",
    "\n",
    "Pillow es una librería ampliamente usada para lectura, escritura y manipulación de imágenes\n",
    "\n",
    "Usamos la transformación [`ToTensor()`](https://pytorch.org/docs/stable/torchvision/transforms.html) para \n",
    "- Convertirlo en tipo torch\n",
    "- Formatearlo a `float32` con píxeles en $[0,1]$\n",
    "- Dimensionarlo como [C, W, H]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtros y Convolución bidimensional\n",
    "\n",
    "Usaremos [`scipy.signal.convolve2d`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html) para visualizar el efecto de algunos filtros simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "\n",
    "filtro = np.array([[1., -1]])\n",
    "#filtro = np.ones(shape=(5, 5))\n",
    "#filtro = np.zeros(shape=(28, 28)); \n",
    "#X, Y = np.meshgrid(np.arange(filtro.shape[0]), np.arange(filtro.shape[1]))\n",
    "#filtro[((X-14)**2 + (Y-14)**2 > 5**2) & ((X-14)**2 + (Y-14)**2 < 7**2) ] = 1\n",
    "if np.sum(filtro) > 0:\n",
    "    filtro = filtro/np.sum(filtro)\n",
    "\n",
    "fig, ax = plt.subplots(1, 11, figsize=(8, 2), tight_layout=True)\n",
    "for k in range(10):\n",
    "    image, label = mnist_train_data[k]\n",
    "    np_image = image.numpy()[0, :, :]\n",
    "    cv_image = np.absolute(scipy.signal.convolve2d(np_image, filtro, mode='same'))\n",
    "    ax[k].imshow(cv_image, cmap=plt.cm.Greys_r, vmin=0, vmax=1)\n",
    "    ax[k].axis('off');\n",
    "    ax[k].set_title(label)\n",
    "\n",
    "ax[10].axis('off')\n",
    "ax[10].imshow(filtro, cmap=plt.cm.RdBu_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternativa: Si tienes un dataset como matriz NumPy puedes usar `TensorDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.datasets import fetch_openml\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "mnist_train_data = TensorDataset(torch.from_numpy(X_train.astype('float32')),\n",
    "                                 torch.from_numpy(y_train.astype('int64')))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch DataLoaders\n",
    "\n",
    "Como vimos la clase pasada necesitamos un `DataLoader` para alimentar nuestras redes neuronales\n",
    "\n",
    "Un `DataLoader` se crea a partir de un torch `DataSet` y opcionalmente un torch `DataSampler`\n",
    "\n",
    "Este último especifica como se extraen los datos (aleatoreo, ponderado, etc)\n",
    "\n",
    "La documentación del modulo `data`: https://pytorch.org/docs/stable/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "import sklearn.model_selection\n",
    "# Usamos sklearn StratifiedShuffleSplit para generar índices de entrenamiento y validación\n",
    "sss = sklearn.model_selection.StratifiedShuffleSplit(train_size=0.6)\n",
    "train_idx, valid_idx = next(sss.split(np.zeros(len(mnist_train_data)), mnist_train_data.targets))\n",
    "# Usamos Subset para crear las particiones a partir de los índices y entregamos los subset a los DataLoaders\n",
    "train_loader = DataLoader(Subset(mnist_train_data, train_idx), shuffle=True, batch_size=32)\n",
    "valid_loader = DataLoader(Subset(mnist_train_data, valid_idx), shuffle=False, batch_size=256)\n",
    "\n",
    "mnist_test_data = torchvision.datasets.MNIST('dataset', train=False, download=True,\n",
    "                                             transform=torchvision.transforms.ToTensor())\n",
    "test_loader = DataLoader(mnist_test_data, shuffle=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales Convolucionales\n",
    "\n",
    "Estudie la documentación de [`torch.nn`](https://pytorch.org/docs/stable/nn.html).\n",
    "\n",
    "Escriba una clase que herede de `torch.nn.Module` para clasificar MNIST (10 clases)\n",
    "\n",
    "Implemente\n",
    "1. Una red convolucional con arquitectura Alexnet\n",
    "1. Agregue dropout\n",
    "1. Aumentación de datos en el conjunto de entrenamiento (puede usar las transformaciones de torchvision)\n",
    "\n",
    "Para cada red\n",
    "- Visualice los filtros obtenidos en las primeras tres capas, ¿puede encontrar los filtros asociados a cada clase?\n",
    "- Mida el costo de entrenamiento y validación\n",
    "- Use early-stopping para detener el entrenamiento\n",
    "- Evalue la red detenida en el conjunto de prueba, obtenga la loss y una tabla de confusión\n",
    "\n",
    "\n",
    "Compare y analice sus resultados \n",
    "\n",
    "Puede usar la librería [hiddenlayer](https://github.com/waleedka/hiddenlayer) o [tensorboard](https://pytorch.org/docs/stable/tensorboard.html) para visualizar el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mi_convnet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(mi_convnet, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(28*28*1, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.hidden(x.reshape(-1, 28*28*1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet = mi_convnet()\n",
    "optimizer = torch.optim.Adam(nnet.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    nnet = nnet.cuda()\n",
    "\n",
    "# Hiddenlayer objects to track metrics\n",
    "import hiddenlayer as hl\n",
    "history1 = hl.History()\n",
    "canvas1 = hl.Canvas()\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "## tensorboard --logdir=/tmp/tensorboard\n",
    "#writer = SummaryWriter(\"/tmp/tensorboard/net1\")\n",
    "\n",
    "nepochs = 5\n",
    "for epoch in range(nepochs): \n",
    "    epoch_loss, epoch_acc = 0.0, 0.0\n",
    "    # Train\n",
    "    for mbdata, mblabel in train_loader:\n",
    "        if use_gpu:\n",
    "            mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "        prediction = nnet(mbdata)\n",
    "        optimizer.zero_grad()        \n",
    "        loss = criterion(prediction, mblabel)  \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (prediction.argmax(dim=1) == mblabel).sum().item()        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    #writer.add_scalar('Train/Loss', epoch_loss/len(train_idx), epoch)\n",
    "    #writer.add_scalar('Train/Acc', epoch_acc/len(train_idx), epoch)\n",
    "    epoch_loss, epoch_acc = 0.0, 0.0\n",
    "    for mbdata, mblabel in valid_loader:\n",
    "        if use_gpu:\n",
    "            mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "        prediction = nnet(mbdata)\n",
    "        loss = criterion(prediction, mblabel)  \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (prediction.argmax(dim=1) == mblabel).sum().item()        \n",
    "    #writer.add_scalar('Valid/Loss', epoch_loss/len(valid_idx), epoch)\n",
    "    #writer.add_scalar('Valid/Acc', epoch_acc/len(valid_idx), epoch)\n",
    "    history1.log(epoch, loss=epoch_loss/len(valid_idx), accuracy=epoch_acc/len(valid_idx))\n",
    "    with canvas1: # So that they render together\n",
    "        canvas1.draw_plot([history1[\"loss\"]])\n",
    "        canvas1.draw_plot([history1[\"accuracy\"]])\n",
    "    #time.sleep(0.1)\n",
    "\n",
    "if use_gpu:\n",
    "    nnet = nnet.cpu()\n",
    "    \n",
    "#writer.add_graph(nnet)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
