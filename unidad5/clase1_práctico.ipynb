{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noisy_xor(N_per_cluster=500, stddev_noise=0.4):\n",
    "    data = stddev_noise*np.random.randn(4*N_per_cluster, 2)\n",
    "    data[0*N_per_cluster:1*N_per_cluster, :] += [1.0, -1.0]\n",
    "    data[1*N_per_cluster:2*N_per_cluster, :] += [-1.0, 1.0]\n",
    "    data[2*N_per_cluster:3*N_per_cluster :] += [-1.0, -1.0]\n",
    "    data[3*N_per_cluster:4*N_per_cluster, :] += [1.0, 1.0]\n",
    "    #data = (data - np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "    labels = np.zeros(shape=(4*N_per_cluster,), dtype=int)\n",
    "    labels[2*N_per_cluster:] = 1.0\n",
    "    NP = np.random.permutation(4*N_per_cluster)\n",
    "    return data[NP, :], labels[NP]\n",
    "\n",
    "# Create training and test set\n",
    "data , labels = create_noisy_xor()\n",
    "data_train, labels_train = data[:500, :], labels[:500]\n",
    "data_test, labels_test = data[500:, :], labels[500:]\n",
    "print(\"%d samples for train and %d for testing\" %(len(data_train), len(data)-len(data_train)))\n",
    "# Plot data\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.scatter(data[labels==0, 0], data[labels==0, 1], marker='o', label=\"class 1\", alpha=0.5)\n",
    "ax.scatter(data[labels==1, 0], data[labels==1, 1], marker='o', label=\"class 2\", alpha=0.5)\n",
    "ax.set_xlabel('X1'); ax.set_ylabel('X2')\n",
    "ax.grid(); plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "class MLP:\n",
    "    \n",
    "    def __init__(self, input_dim=2, hidden_dim=10, rstate=None):\n",
    "        np.random.seed(rstate)        \n",
    "        assert hidden_dim >0, \"Neuronas ocultas debe ser mayor que cero\"\n",
    "        self.hidden_dim = hidden_dim\n",
    "        output_dim = 1 # Clasificación binaria\n",
    "        self.hidden_params = {'w':np.random.randn(input_dim, hidden_dim),\n",
    "                              'b':np.random.randn(hidden_dim)}\n",
    "        self.output_params = {'w': np.random.randn(hidden_dim, output_dim),\n",
    "                              'b': np.random.randn(output_dim)} \n",
    "\n",
    "    def forward(self, x, only_output=True):\n",
    "        z = logistic(np.dot(x, self.hidden_params['w']) + self.hidden_params['b'])  \n",
    "        y = logistic(np.dot(z, self.output_params['w']) + self.output_params['b'])\n",
    "        if only_output:\n",
    "            return y\n",
    "        else:\n",
    "            return z, y\n",
    "        \n",
    "    def score(self, x, y, eps=1e-10):        \n",
    "        yhat = self.forward(x)[:, 0] \n",
    "        logL = y*np.log(yhat+eps) + (1.0-y)*np.log(1.0-yhat+eps)\n",
    "        return -logL\n",
    "    \n",
    "    def backward(self, x, y, eta=1e-2):\n",
    "        zhat, yhat = self.forward(x, only_output=False)\n",
    "        dL = -(y - yhat)  # La derivada de -logL/dyhat x dlogistic\n",
    "        self.output_params['w'] -= eta*np.dot(zhat.T, dL)\n",
    "        self.output_params['b'] -= eta*np.sum(dL)        \n",
    "        grad_z = dL*np.repeat(self.output_params['w'].T, len(dL), axis=0)*zhat*(1-zhat)\n",
    "        self.hidden_params['w'] -= eta*np.dot(x.T, grad_z)\n",
    "        self.output_params['b'] -= eta*np.sum(grad_z)\n",
    "        \n",
    "\n",
    "# Modifique la red para hacer regresión en ves de clasificación\n",
    "# Modifique la red para usar sigmoide en vez de tangente hiperbólica en la capa oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet = MLP(hidden_dim=2)\n",
    "n_epochs, eta = 1000, 1e-3\n",
    "cost_history = np.zeros(shape=(n_epochs, 2))\n",
    "x_min, x_max = data[:, 0].min() - 0.5, data[:, 0].max() + 0.5\n",
    "y_min, y_max = data[:, 1].min() - 0.5, data[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4), tight_layout=True)\n",
    "\n",
    "def update_plot(k):\n",
    "    global nnet, cost_history    \n",
    "    cost_history[k, 0] = np.mean(nnet.score(data_train, labels_train))\n",
    "    cost_history[k, 1] = np.mean(nnet.score(data_test, labels_test))\n",
    "    [ax_.cla() for ax_ in ax]\n",
    "    Z = nnet.forward(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax[0].contourf(xx, yy, Z, cmap=plt.cm.RdBu_r, alpha=0.75)\n",
    "    for i, (marker, label) in enumerate(zip(['o', 'x'], ['Train', 'Test'])):\n",
    "        ax[0].scatter(data[labels==i, 0], data[labels==i, 1], color='k', marker=marker, alpha=0.5)\n",
    "        ax[1].plot(np.arange(0, k+1, step=1), cost_history[:k+1, i], '-', label=label+\" cost\")\n",
    "    plt.legend(); ax[1].grid()\n",
    "    idx = np.random.permutation(len(data_train))\n",
    "    for i in range(len(idx)//10):\n",
    "        idx_mb = idx[i*10:(i+1)*10]\n",
    "        nnet.backward(data_train[idx_mb, :], labels_train[idx_mb, np.newaxis], eta=eta)\n",
    "\n",
    "anim = animation.FuncAnimation(fig, update_plot, frames=n_epochs, \n",
    "                               interval=10, repeat=False, blit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios**\n",
    "- Experimente variando el número de capas, número de neuronas y tasa de aprendizaje. Comente sobre como se reflejan estas modificaciones en el desempeño de la red y en la  complejidad del hiperplano\n",
    "- Experimente aumentando el ruido de los datos\n",
    "- Discuta sobre la relación entre complejidad del hiperplano, capacidad de generalización y sobreajuste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación usando pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** Modifique la clase para agregando una segunda capa oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset \n",
    "\n",
    "torch_train_set = TensorDataset(torch.from_numpy(data_train.astype('float32')), \n",
    "                                torch.from_numpy(labels_train.astype('float32')))\n",
    "torch_train_loader = DataLoader(torch_train_set, shuffle=True, batch_size=32)\n",
    "\n",
    "class myMLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=2, hidden_dim=10, output_dim=1):        \n",
    "        super(myMLP, self).__init__()        \n",
    "        self.fc1 = torch.nn.Linear(in_features=input_dim,  out_features=hidden_dim, bias=True)\n",
    "        self.fc2 = torch.nn.Linear(in_features=hidden_dim, out_features=output_dim, bias=True)\n",
    "        self.hidden_activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.hidden_activation(self.fc1(x))\n",
    "        y = self.fc2(z)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** Modifique el siguiente código para correr en GPU. \n",
    "\n",
    "Por qué no se ve un speed-up considerable en este caso?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  tqdm import tqdm_notebook\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = myMLP(dim_input=2, dim_hidden=10, dim_classes=1)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "#net.to(device)\n",
    "\n",
    "# Main training loop\n",
    "nepoch = 500\n",
    "running_loss = np.zeros(shape=(nepoch,))\n",
    "for k in tqdm_notebook(range(nepoch), desc='Epochs'): \n",
    "    for sample_data, sample_label in torch_train_loader:\n",
    "        output = net(sample_data)\n",
    "        optimizer.zero_grad()        \n",
    "        loss = criterion(output[:, 0], sample_label)  \n",
    "        running_loss[k] += loss.item()/torch_train_loader.__len__()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = net.forward(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))\n",
    "Z = Z.detach().numpy().reshape(xx.shape)\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.contourf(xx, yy, Z, cmap=plt.cm.RdBu_r, alpha=0.75)\n",
    "ax.scatter(data[labels==0, 0], data[labels==0, 1], color='k', marker='o', alpha=0.5)\n",
    "ax.scatter(data[labels==1, 0], data[labels==1, 1], color='k', marker='x', alpha=0.5)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(running_loss, label='Train loss', linewidth=2)\n",
    "#ax.plot(test_loss, label='Test loss', linewidth=2)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opcional: Implementación usando tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "from os.path import join\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf_input = tf.placeholder(tf.float32, [None, 2], name='input')\n",
    "tf_label = tf.placeholder(tf.float32, [None, 1], name='target')\n",
    "\n",
    "Nh = 10\n",
    "nepochs = 500  \n",
    "\n",
    "with tf.variable_scope('Hidden_layer'):\n",
    "    bh = tf.Variable(tf.zeros([Nh]), name=\"bias\", dtype=tf.float32)\n",
    "    wh = tf.Variable(tf.random_uniform([2, Nh], -1.0, 1.0), name=\"weight\", dtype=tf.float32)\n",
    "    z = tf.nn.tanh(tf.matmul(tf_input, wh) + bh)\n",
    "\n",
    "with tf.variable_scope('Output_layer'):\n",
    "    bo = tf.Variable(tf.zeros([1]), name=\"bias\", dtype=tf.float32)\n",
    "    wo = tf.Variable(tf.random_uniform([Nh, 1], -1.0, 1.0), name=\"weight\", dtype=tf.float32)\n",
    "    y = tf.add(tf.matmul(z, wo), bo)\n",
    "\n",
    "with tf.variable_scope('Optimizer'):\n",
    "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_label, logits=y)\n",
    "    loss_op = tf.reduce_mean(cross_entropy)  \n",
    "    optimizer = tf.train.AdamOptimizer(1e-2)\n",
    "    train_op = optimizer.minimize(loss_op) \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss_op)\n",
    "    tf.summary.histogram('output_weight', wo)\n",
    "    tf.summary.histogram('output_bias', bo)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios:** \n",
    "- Visualice el grafo, las curvas de aprendizaje y los histogramas de parámetros usando la herramienta tensorboard\n",
    "- Modifique el código que genera el grafo para agregar una segunda capa oculta\n",
    "- Estudie la función de mayor abstracción tf.layers.dense y usela para modificar el código que genera el grafo\n",
    "- ¿Cómo modificaría el código de entrenamiento para usar mini-batches?\n",
    "\n",
    "**Instrucciones tensorboard**\n",
    "1. Ejecutar: tensorboard --logdir /tmp/tensorboard/ \n",
    "2. Apuntar el navegador a localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = join(\"/tmp/tensorboard/\", str(time.time()))\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(join(log_dir, 'train'), sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(join(log_dir, 'test'), sess.graph)\n",
    "    sess.run(init)\n",
    "    train_loss = np.zeros(shape=(nepochs))\n",
    "    test_loss = np.zeros(shape=(nepochs))\n",
    "    for i, epoch in enumerate(range(nepochs)):\n",
    "        # run the training operation\n",
    "        _, train_loss[i], summary = sess.run([train_op, loss_op, merged], feed_dict={tf_input: data_train, \n",
    "                                                         tf_label: np.reshape(labels_train, [-1, 1])})\n",
    "        train_writer.add_summary(summary, i)\n",
    "        pred_test, test_loss[i], summary = sess.run([y, loss_op, merged], feed_dict={tf_input: data_test, \n",
    "                                                                 tf_label: np.reshape(labels_test, [-1, 1])})\n",
    "        test_writer.add_summary(summary, i)\n",
    "\n",
    "    Z = sess.run(y, feed_dict={tf_input: (np.c_[xx.ravel(), yy.ravel()]).astype('float32')})\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.contourf(xx, yy, Z, cmap=plt.cm.RdBu_r, alpha=0.75)\n",
    "ax.scatter(data[labels==0, 0], data[labels==0, 1], color='k', marker='o', alpha=0.5)\n",
    "ax.scatter(data[labels==1, 0], data[labels==1, 1], color='k', marker='x', alpha=0.5)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(train_loss, label='Train loss', linewidth=2)\n",
    "ax.plot(test_loss, label='Test loss', linewidth=2)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensorboard: análisis del grafo, curvas de aprendizaje, histograma de parámetros"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
