{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = sklearn.datasets.make_circles(n_samples=1000, noise=0.2, factor=0.25)\n",
    "data, labels = sklearn.datasets.make_moons(n_samples=1000, noise=0.2)\n",
    "data, labels = sklearn.datasets.make_blobs(n_samples=[250]*4, n_features=2, cluster_std=0.5,\n",
    "                                          centers=np.array([[-1, 1], [1, 1], [-1, -1], [1, -1]]))\n",
    "labels[labels==2] = 1; labels[labels==3] = 0;\n",
    "\n",
    "train_idx, test_idx = next(sklearn.model_selection.ShuffleSplit(train_size=0.6).split(data, labels))\n",
    "print(\"%d samples for train and %d for testing\" %(len(train_idx), len(data)-len(train_idx)))\n",
    "# Plot data\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.scatter(data[labels==0, 0], data[labels==0, 1], marker='o', label=\"class 1\", alpha=0.5)\n",
    "ax.scatter(data[labels==1, 0], data[labels==1, 1], marker='o', label=\"class 2\", alpha=0.5)\n",
    "ax.set_xlabel('X1'); ax.set_ylabel('X2')\n",
    "ax.grid(); plt.legend();\n",
    "\n",
    "x_min, x_max = data[:, 0].min() - 0.5, data[:, 0].max() + 0.5\n",
    "y_min, y_max = data[:, 1].min() - 0.5, data[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red neuronal MLP con numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "class MLP:\n",
    "    \n",
    "    def __init__(self, input_dim=2, hidden_dim=10, rstate=None):\n",
    "        np.random.seed(rstate)        \n",
    "        assert hidden_dim >0, \"Neuronas ocultas debe ser mayor que cero\"\n",
    "        self.hidden_dim = hidden_dim\n",
    "        output_dim = 1 # Clasificación binaria\n",
    "        self.hidden_params = {'w':np.random.randn(input_dim, hidden_dim),\n",
    "                              'b':np.random.randn(hidden_dim)}\n",
    "        self.output_params = {'w': np.random.randn(hidden_dim, output_dim),\n",
    "                              'b': np.random.randn(output_dim)} \n",
    "\n",
    "    def forward(self, x, only_output=True):\n",
    "        z = logistic(np.dot(x, self.hidden_params['w']) + self.hidden_params['b'])  \n",
    "        y = logistic(np.dot(z, self.output_params['w']) + self.output_params['b'])\n",
    "        if only_output:\n",
    "            return y\n",
    "        else:\n",
    "            return z, y\n",
    "        \n",
    "    def score(self, x, y, eps=1e-10):        \n",
    "        yhat = self.forward(x)[:, 0] \n",
    "        logL = y*np.log(yhat+eps) + (1.0-y)*np.log(1.0-yhat+eps)\n",
    "        return -logL\n",
    "    \n",
    "    def backward(self, x, y, eta=1e-2):\n",
    "        zhat, yhat = self.forward(x, only_output=False)\n",
    "        # ¿A que corresponde la ecuación siguiente?\n",
    "        dL = -(y - yhat)  \n",
    "        self.output_params['w'] -= eta*np.dot(zhat.T, dL)\n",
    "        self.output_params['b'] -= eta*np.sum(dL) \n",
    "        # ¿A que corresponde la ecuación siguiente?\n",
    "        grad_z = dL*np.repeat(self.output_params['w'].T, len(dL), axis=0)*zhat*(1-zhat)\n",
    "        self.hidden_params['w'] -= eta*np.dot(x.T, grad_z)\n",
    "        self.output_params['b'] -= eta*np.sum(grad_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet = MLP(hidden_dim=10)\n",
    "n_epochs, eta = 1000, 1e-2\n",
    "cost_history = np.zeros(shape=(n_epochs, 2))\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4), tight_layout=True)\n",
    "\n",
    "def update_plot(k):\n",
    "    global nnet, cost_history    \n",
    "    cost_history[k, 0] = np.mean(nnet.score(data[train_idx], labels[train_idx]))\n",
    "    cost_history[k, 1] = np.mean(nnet.score(data[test_idx], labels[test_idx]))\n",
    "    [ax_.cla() for ax_ in ax]\n",
    "    Z = nnet.forward(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax[0].contourf(xx, yy, Z, cmap=plt.cm.RdBu_r, alpha=1., vmin=0, vmax=1)\n",
    "    for i, (marker, label) in enumerate(zip(['o', 'x'], ['Train', 'Test'])):\n",
    "        ax[0].scatter(data[labels==i, 0], data[labels==i, 1], s=10, color='k', marker=marker, alpha=0.5)\n",
    "        ax[1].plot(np.arange(0, k+1, step=1), cost_history[:k+1, i], '-', label=label+\" cost\")\n",
    "    plt.legend(); ax[1].grid()\n",
    "    idx = np.random.permutation(len(train_idx))\n",
    "    for i in range(len(idx)//10):\n",
    "        idx_mb = train_idx[idx[i*10:(i+1)*10]]\n",
    "        nnet.backward(data[idx_mb, :], labels[idx_mb, np.newaxis], eta=eta)\n",
    "\n",
    "anim = animation.FuncAnimation(fig, update_plot, frames=n_epochs, \n",
    "                               interval=10, repeat=False, blit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios**\n",
    "- Experimente variando el número de capas, número de neuronas y tasa de aprendizaje. Comente sobre como se reflejan estas modificaciones en el desempeño de la red y en la  complejidad del hiperplano\n",
    "- Discuta sobre la relación entre complejidad del hiperplano, capacidad de generalización y sobreajuste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red neuronal MLP con [PyTorch](https://pytorch.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Implementar red neuronal\n",
    "class myMLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=2, hidden_dim=10, output_dim=1):        \n",
    "        super(myMLP, self).__init__()  \n",
    "        # Completar aquí\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Completar aquí\n",
    "\n",
    "# Crear conjuntos de entrenamiento y prueba\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset \n",
    "\n",
    "torch_set = TensorDataset(torch.from_numpy(data.astype('float32')), \n",
    "                          torch.from_numpy(labels.astype('float32')))\n",
    "\n",
    "torch_train_loader = DataLoader(Subset(torch_set, train_idx), shuffle=True, batch_size=32)\n",
    "torch_valid_loader = DataLoader(Subset(torch_set, test_idx), shuffle=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3.5), tight_layout=True)\n",
    "\n",
    "n_epochs = 1000\n",
    "net = myMLP(input_dim=2, hidden_dim=100, output_dim=1)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "running_loss = np.zeros(shape=(n_epochs, 2))\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "def train_one_epoch(net):\n",
    "    train_loss, valid_loss = 0.0, 0.0\n",
    "    for sample_data, sample_label in torch_train_loader:\n",
    "        output = net(sample_data)\n",
    "        optimizer.zero_grad()        \n",
    "        loss = criterion(output[:, 0], sample_label)  \n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    for sample_data, sample_label in torch_valid_loader:\n",
    "        output = net(sample_data)\n",
    "        loss = criterion(output[:, 0], sample_label)  \n",
    "        valid_loss += loss.item()\n",
    "    return train_loss/torch_train_loader.__len__(), valid_loss/torch_valid_loader.__len__()\n",
    "    \n",
    "def update_plot(k):\n",
    "    global net, running_loss\n",
    "    [ax_.cla() for ax_ in ax]\n",
    "    running_loss[k, 0], running_loss[k, 1] = train_one_epoch(net)\n",
    "    Z = net.forward(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))\n",
    "    Z = sigmoid(Z).detach().numpy().reshape(xx.shape)\n",
    "    ax[0].contourf(xx, yy, Z, cmap=plt.cm.RdBu_r, alpha=1., vmin=0, vmax=1)\n",
    "    for i, (marker, label) in enumerate(zip(['o', 'x'], ['Train', 'Test'])):\n",
    "        ax[0].scatter(data[labels==i, 0], data[labels==i, 1], color='k', s=10, marker=marker, alpha=0.5)\n",
    "        ax[1].plot(np.arange(0, k+1, step=1), running_loss[:k+1, i], '-', label=label+\" cost\")\n",
    "    plt.legend(); ax[1].grid()\n",
    "    \n",
    "anim = animation.FuncAnimation(fig, update_plot, frames=n_epochs, \n",
    "                               interval=10, repeat=False, blit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opcional: Implementación usando tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "from os.path import join\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf_input = tf.placeholder(tf.float32, [None, 2], name='input')\n",
    "tf_label = tf.placeholder(tf.float32, [None, 1], name='target')\n",
    "\n",
    "Nh = 10\n",
    "nepochs = 500  \n",
    "\n",
    "with tf.variable_scope('Hidden_layer'):\n",
    "    bh = tf.Variable(tf.zeros([Nh]), name=\"bias\", dtype=tf.float32)\n",
    "    wh = tf.Variable(tf.random_uniform([2, Nh], -1.0, 1.0), name=\"weight\", dtype=tf.float32)\n",
    "    z = tf.nn.tanh(tf.matmul(tf_input, wh) + bh)\n",
    "\n",
    "with tf.variable_scope('Output_layer'):\n",
    "    bo = tf.Variable(tf.zeros([1]), name=\"bias\", dtype=tf.float32)\n",
    "    wo = tf.Variable(tf.random_uniform([Nh, 1], -1.0, 1.0), name=\"weight\", dtype=tf.float32)\n",
    "    y = tf.add(tf.matmul(z, wo), bo)\n",
    "\n",
    "with tf.variable_scope('Optimizer'):\n",
    "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_label, logits=y)\n",
    "    loss_op = tf.reduce_mean(cross_entropy)  \n",
    "    optimizer = tf.train.AdamOptimizer(1e-2)\n",
    "    train_op = optimizer.minimize(loss_op) \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss_op)\n",
    "    tf.summary.histogram('output_weight', wo)\n",
    "    tf.summary.histogram('output_bias', bo)\n",
    "\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios:** \n",
    "- Visualice el grafo, las curvas de aprendizaje y los histogramas de parámetros usando la herramienta tensorboard\n",
    "- Modifique el código que genera el grafo para agregar una segunda capa oculta\n",
    "- Estudie la función de mayor abstracción tf.layers.dense y usela para modificar el código que genera el grafo\n",
    "- ¿Cómo modificaría el código de entrenamiento para usar mini-batches?\n",
    "\n",
    "**Instrucciones tensorboard**\n",
    "1. Ejecutar: tensorboard --logdir /tmp/tensorboard/ \n",
    "2. Apuntar el navegador a localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = join(\"/tmp/tensorboard/\", str(time.time()))\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(join(log_dir, 'train'), sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(join(log_dir, 'test'), sess.graph)\n",
    "    sess.run(init)\n",
    "    train_loss = np.zeros(shape=(nepochs))\n",
    "    test_loss = np.zeros(shape=(nepochs))\n",
    "    for i, epoch in enumerate(range(nepochs)):\n",
    "        # run the training operation\n",
    "        _, train_loss[i], summary = sess.run([train_op, loss_op, merged], feed_dict={tf_input: data_train, \n",
    "                                                         tf_label: np.reshape(labels_train, [-1, 1])})\n",
    "        train_writer.add_summary(summary, i)\n",
    "        pred_test, test_loss[i], summary = sess.run([y, loss_op, merged], feed_dict={tf_input: data_test, \n",
    "                                                                 tf_label: np.reshape(labels_test, [-1, 1])})\n",
    "        test_writer.add_summary(summary, i)\n",
    "\n",
    "    Z = sess.run(y, feed_dict={tf_input: (np.c_[xx.ravel(), yy.ravel()]).astype('float32')})\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.contourf(xx, yy, Z, cmap=plt.cm.RdBu_r, alpha=0.75)\n",
    "ax.scatter(data[labels==0, 0], data[labels==0, 1], color='k', marker='o', alpha=0.5)\n",
    "ax.scatter(data[labels==1, 0], data[labels==1, 1], color='k', marker='x', alpha=0.5)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(train_loss, label='Train loss', linewidth=2)\n",
    "ax.plot(test_loss, label='Test loss', linewidth=2)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
